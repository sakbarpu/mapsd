From githubarchive.com :::


	Open-source developers all over the world are working on millions of projects:
	writing code & documentation, fixing & submitting bugs, and so forth. GH
	Archive is a project to record the public GitHub timeline, archive it, and make
	it easily accessible for further analysis.

	The entire GH Archive is also available as a public dataset on Google BigQuery:
	the dataset is automatically updated every hour and enables you to run
	arbitrary SQL-like queries over the entire dataset in seconds. To get started:

	    If you don't already have a Google project...
		Login into the Google Developer Console
		Create a project and activate the BigQuery API

	    Open public dataset: https://bigquery.cloud.google.com/table/githubarchive:day.20150101
	    The above link to open dataset is not valid
	    Instead I had it opened here https://console.cloud.google.com/bigquery?project=githubarchive-245805&supportedpurview=project&page=jobs



From stackoverflow answer https://stackoverflow.com/questions/7329978/how-to-list-all-github-users :::

	The query I exectuted in BIGQUERY google is:

	SELECT actor.login 
	FROM [githubarchive:year.2018]

	Gets you actor.logins or usernames for all the users in GitHub that logged in in year 2018.




From stackoverflow answer https://stackoverflow.com/questions/18493533/how-to-download-all-data-in-a-google-bigquery-dataset/37274820#37274820

	enable billing

	You have to give your credit card number to Google to export the output, and you might have to pay.

	But the free quota (1TB of processed data) should suffice for many hobby projects.

	create a project

	associate billing to a project

	do your query

	create a new dataset

	click "Show options" and enable "Allow Large Results" if the output is very large

	export the query result to a table in the dataset

	create a bucket on Cloud Storage.

	export the table to the created bucked on Cloud Storage.

	    make sure to click GZIP compression

	    use a name like <bucket>/prefix.gz.

	    If the output is very large, the file name must have an asterisk * and the output will be split into multiple files.

	download the table from cloud storage to your computer.

	Does not seem possible to download multiple files from the web interface if the large file got split up, but you could install gsutil and run:

	gsutil -m cp -r 'gs://<bucket>/prefix_*' .

	See also: Download files and folders from Google Storage bucket to a local folder

	There is a gsutil in Ubuntu 16.04 but it is an unrelated package.

	Gunzip the files downloaded from the bucket
